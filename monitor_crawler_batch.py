import time
import csv
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def crawl_monitor_list(crawling_url, max_page=100):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument("lang=ko_KR")
    options.add_argument("user-agent=Mozilla/5.0")

    driver = webdriver.Chrome(options=options)
    wait = WebDriverWait(driver, 10)

    driver.get(crawling_url)
    time.sleep(2)

    try:
        driver.find_element(By.XPATH, '//option[@value="90"]').click()
        wait.until(EC.invisibility_of_element_located((By.CLASS_NAME, 'product_list_cover')))
    except:
        pass

    results = {}

    # ì‹¤ì œ ì‹ ìƒí’ˆ ì •ë ¬ì€ 'ì‹ ìƒí’ˆìˆœ' í•„í„° í´ë¦­ í•„ìš” (ì˜ˆ: ì •ë ¬ ê¸°ì¤€ = ë“±ë¡ì¼)
    try:
        sort_button = driver.find_element(By.XPATH, '//li[@data-sort-method="releaseDate"]')
        sort_button.click()
        wait.until(EC.invisibility_of_element_located((By.CLASS_NAME, 'product_list_cover')))
        print("ğŸ”€ ì •ë ¬ê¸°ì¤€: ì‹ ìƒí’ˆìˆœ (ë“±ë¡ì¼ ìˆœ)")
    except:
        print("âš ï¸ ì‹ ìƒí’ˆìˆœ ì •ë ¬ ì‹¤íŒ¨, ê¸°ë³¸ ì •ë ¬ ìœ ì§€")

    page = 1
    while page <= max_page:
        print(f"ğŸ“„ {page}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
        try:
            wait.until(EC.presence_of_all_elements_located((By.XPATH, '//ul[@class="product_list"]/li')))
        except:
            print("â³ ë¡œë”© ì‹¤íŒ¨")
            break

        time.sleep(1)
        products = driver.find_elements(By.XPATH, '//ul[@class="product_list"]/li')

        for product in products:
            try:
                pid = product.get_attribute("id")
                if not pid or "ad" in pid:
                    continue
                product_id = pid.replace("productItem", "")
                if product_id in results:
                    continue

                model_name = product.find_element(By.XPATH, './div/div[2]/p/a').text.strip()

                price = "ê°€ê²©ì—†ìŒ"
                try:
                    price = product.find_element(By.CSS_SELECTOR, 'p.price_sect strong').text.replace(",", "").strip()
                except:
                    try:
                        price = product.find_element(By.CSS_SELECTOR, 'ul > li.mall_list_item > a > p.price_sect > strong').text.replace(",", "").strip()
                    except:
                        pass

                results[product_id] = {
                    "ìƒí’ˆì½”ë“œ": product_id,
                    "ëª¨ë¸ëª…": model_name,
                    "ê°€ê²©": price
                }
            except:
                continue

        # AJAX ê¸°ë°˜ í˜ì´ì§€ ì „í™˜: í˜ì´ì§€ ë²ˆí˜¸ ë²„íŠ¼ í´ë¦­
        try:
            page_buttons = driver.find_elements(By.XPATH, '//a[contains(@class, "num")]')
            for btn in page_buttons:
                if btn.text.strip() == str(page + 1):
                    btn.click()
                    page += 1
                    time.sleep(2)
                    break
            else:
                break  # ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        except:
            break

    driver.quit()

    df = pd.DataFrame(results.values())
    df.to_csv("monitor_list.csv", index=False, encoding="utf-8-sig")
    print(f"âœ… monitor_list.csv ì €ì¥ ì™„ë£Œ (ì´ {len(df)}ê°œ ì œí’ˆ)")

if __name__ == "__main__":
    url = "https://prod.danawa.com/list/?cate=112757"
    crawl_monitor_list(url)
