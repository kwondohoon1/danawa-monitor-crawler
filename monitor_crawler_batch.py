import time
import csv
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def crawl_monitor_list(crawling_url, max_page=200):
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')

    driver = webdriver.Chrome(options=options)
    wait = WebDriverWait(driver, 10)

    driver.get(crawling_url)
    time.sleep(2)

    # í˜ì´ì§€ë‹¹ 90ê°œ ì¶œë ¥ ì„¤ì •
    try:
        driver.find_element(By.XPATH, '//option[@value="90"]').click()
        wait.until(EC.invisibility_of_element((By.CLASS_NAME, 'product_list_cover')))
    except:
        print("âŒ 90ê°œ ì„¤ì • ì‹¤íŒ¨")
        driver.quit()
        return

    results = []
    seen_ids = set()

    for i in range(1, max_page + 1):
        print(f"ğŸ“„ {i}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")
        try:
            wait.until(EC.invisibility_of_element((By.CLASS_NAME, 'product_list_cover')))
            time.sleep(1)

            products = driver.find_elements(By.XPATH, '//ul[@class="product_list"]/li')

            for product in products:
                try:
                    if not product.get_attribute("id") or "ad" in product.get_attribute("id"):
                        continue

                    product_id = product.get_attribute("id").replace("productItem", "")
                    if product_id in seen_ids:
                        continue
                    seen_ids.add(product_id)

                    model_name = product.find_element(By.XPATH, './div/div[2]/p/a').text.strip()

                    # ê°€ê²© íŒŒì‹±
                    price = "ê°€ê²©ì—†ìŒ"
                    try:
                        price = product.find_element(By.CSS_SELECTOR, 'p.price_sect strong').text.replace(",", "").strip()
                    except:
                        try:
                            price = product.find_element(By.CSS_SELECTOR, 'ul > li.mall_list_item > a > p.price_sect > strong').text.replace(",", "").strip()
                        except:
                            pass

                    results.append({
                        "ìƒí’ˆì½”ë“œ": product_id,
                        "ëª¨ë¸ëª…": model_name,
                        "ê°€ê²©": price
                    })
                except Exception as e:
                    print("âš ï¸ ì œí’ˆ íŒŒì‹± ì¤‘ ì˜¤ë¥˜:", str(e))
                    continue

            # ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì‹œë„
            try:
                next_button = driver.find_element(By.XPATH, f'//a[@class="num " and text()="{i + 1}"]')
                next_button.click()
            except:
                try:
                    driver.find_element(By.XPATH, '//a[@class="edge_nav nav_next"]').click()
                except:
                    print("ğŸ”š ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬ ë˜ëŠ” ë” ì´ìƒ í˜ì´ì§€ ì—†ìŒ")
                    break

        except Exception as e:
            print(f"âŒ {i}í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            break

    driver.quit()

    df = pd.DataFrame(results)
    df.to_csv("monitor_list.csv", index=False, encoding="utf-8-sig")
    print(f"âœ… ì´ {len(df)}ê°œ ì œí’ˆ ì €ì¥ ì™„ë£Œ â†’ monitor_list.csv")

if __name__ == "__main__":
    url = "https://prod.danawa.com/list/?cate=112757"
    crawl_monitor_list(url)
